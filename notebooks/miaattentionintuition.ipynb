{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Comprendiendo la Ecuación de Atención en la Arquitectura Transformer\n",
        "Análisis de la inspiración detras de la Ecuación de Atención y sus componentes  en la Arquitectura Transformer, así como una implementación para su uso potencial en el modelo del lenguaje MIA."
      ],
      "metadata": {
        "id": "0s0a2LlYAoE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalación de libreria Numpy como pre-requisito\n",
        "Ejecutar la siguiente celda para instalar la libreria numpy."
      ],
      "metadata": {
        "id": "5AeCJCVqBtUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTQg5ZaxmPAS",
        "outputId": "736d3953-458e-4c25-bdd8-f539ab1f0c99"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Origen de los componentes que conforman la Ecuación de Atención\n",
        "Los algoritmos de atención tomarón prestado los **conceptos de búsqueda de los Sistemas de Recuperación de Información** y lo empacaron como una **capa dentro de la Arquitectura Transformer**. El siguiente bloque de código muestra un tipo de búsqueda que encontramos conmunmente en muchos programas, busquedas exactas sobre la estructura de datos conocida como mapa. De igual forma muestra los principales componentes de este tipo de búsqueda:\n",
        "\n",
        "*   **Q**(uery): La palabra utilizada como criterio de búsqueda.\n",
        "*   **K**(keys): Un conjunto de llaves que forman parte de la estructura mapa y que mapea cada una de estas (1 a 1) con un valor correspondiente.\n",
        "*   **V**(alues): Los valores que corresponden a cada llave en el mapa.\n",
        "\n",
        "No es casual que la mátrices de pesos mencionadas en la Arquitectura Transformer utilicen las letras **Q, K, y V**.\n",
        "Recordemos que la Arquitectura Transformer proviene de los laboratorios de Google y si existe una empresa que sabe de búsquedas es precisamente este coorporativo."
      ],
      "metadata": {
        "id": "NTjQ4y8cyyDR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOhuuDP_Fk0P",
        "outputId": "42936065-03fc-4f67-e8bf-bba2e5ca0b45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n",
            "dict_keys(['méxico', 'alemania', 'inglaterra', 'perro'])\n",
            "dict_values([7, 10, 8, 5])\n"
          ]
        }
      ],
      "source": [
        "# Map of data, where normally exact search are executed in order to look for values\n",
        "# associated with specific keys.\n",
        "\n",
        "data = {\n",
        "    \"méxico\": 7,\n",
        "    \"alemania\": 10,\n",
        "    \"inglaterra\": 8,\n",
        "    \"perro\": 5\n",
        "}\n",
        "\n",
        "query = \"méxico\"\n",
        "print(data[query])\n",
        "\n",
        "print(data.keys())\n",
        "print(data.values())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ecuación de Atención y su relación con búsquedas por significado\n",
        "Siguiendo la intuición detrás del algoritmo de atención, las búsquedas basados en el significado de la palabra consultada Q(uery) en lugar de búsquedas exactas son fundamentales para el algoritmo porque el resultado arrojara conceptos similares. El siguiente bloque de código muestra como podríamos realizar una búsqueda por significado utilizando **pesos (que tan importante es la llave K(eys) con respecto a la palabra utilizada como criterio Q(uery), la puntuación de atención)** y realizando una suma ponderada. Como seres humanos facilmente podemos ver que las 3 primeras llaves representan países y por tanto si buscamos por el concepto de \"país\" debemos estas llaves deben ser relevantes para los resultados que entreguemos (debemos prestar atención a las mismas) por lo que le **asignamos un peso de nuestra elección a cada una de ellas de acuerdo a que tan cerca esta la K(ey) con respecto a la palabra o Q(uery) consultada**."
      ],
      "metadata": {
        "id": "ZW2I-Jk31jjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The intuition behind the attention mechanism defined in original transformer\n",
        "# architecture is a search based in meaning instead of exact search taking advantage\n",
        "# of dot product between vectors or matrix.\n",
        "# This code only represent a weighted sum of attention scores (how similar we think)\n",
        "# each key is from the provided query returning a value.\n",
        "\n",
        "query = \"país\"\n",
        "0.30 * data[\"méxico\"] + 0.30 * data[\"alemania\"] + 0.30 * data[\"inglaterra\"] + 0.10 * data[\"perro\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gDMEeeDIApu",
        "outputId": "359e18f4-943c-49af-96cd-6895d860dd92"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Busquedas por significado y su relación con Embeddings\n",
        "Los **Embeddings** de una palabra ***son vectores compactos con una dimensionalidad que son fundamentales en todo lo que se refiere al procesamiento de lenguaje natural incluidos los Modelos de Lenguaje Grandes y la arquitectura sobre la que se basan, la Arquitectura Transformer***. El siguiente bloque de código muestra un ejemplo de **búsqueda semántica** aprovechando la **Ecuación de Atención** definida en la Arquitectura Transformer; **esta es la ecuación utilizada para determinar la atención o dicho de otra manera los pesos que debemos asignar a cada combinación de palabras entre la consultada Q(uery) y los K(eys)** y que en la sección anterior elegimos de acuerdo a nuestra propia comprensión del lenguaje."
      ],
      "metadata": {
        "id": "-7wzOOyn4xfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def getWordEmbedding(word: str, embdim: int = 8):\n",
        "    \"\"\"\n",
        "    Returns a normalized vector of provided dimension (by default 8) that emulate\n",
        "    an embedding for the word. In a real implementation the word embedding should\n",
        "    come from a real implementation.\n",
        "\n",
        "    Parameters:\n",
        "        word (string): Word to get embedding.\n",
        "        embdim (int): Embedding dimesion, by default 8 when it is not provided.\n",
        "\n",
        "    Returns:\n",
        "        A vector that represent the embedding for the word.\n",
        "    \"\"\"\n",
        "    return np.random.normal(size=(embdim))\n",
        "\n",
        "def computeSoftmax(x):\n",
        "    \"\"\"\n",
        "    Naive implementation of the softmax function using numpy library.\n",
        "\n",
        "    Parameters:\n",
        "        x (vector): Embedding vector that represent a word.\n",
        "\n",
        "    Returns:\n",
        "        A vector with normalized values for x.\n",
        "    \"\"\"\n",
        "    return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "def computeAttention(q, k, v):\n",
        "    \"\"\"\n",
        "    Compute attention scores for query vector (an embedding vector).\n",
        "\n",
        "    Parameters:\n",
        "        q (vector): Embedding vector that represent a word.\n",
        "        keys (list): A list of keys being compared with the query vector based\n",
        "        in its similarity. Internally the transpose of this parameter is being used.\n",
        "        values (list): Each value being mapped by the keys.\n",
        "\n",
        "    Returns:\n",
        "        A matrix with all the computed attention scores.\n",
        "    \"\"\"\n",
        "\n",
        "    return computeSoftmax(q @ k.T) @ v\n",
        "\n",
        "def searchByMeaning(queryWord: str, keys, values):\n",
        "    \"\"\"\n",
        "    Search for the query word by its meaning using the concept of embeddings similarity\n",
        "    result of a dot product and assigning a weight of attention to each combination of\n",
        "    the queried word with provided keys.\n",
        "\n",
        "    Parameters:\n",
        "        queryword (string): Queried word. Internally an embeding of it is obtained\n",
        "        before to evaluate the attention equation.\n",
        "        keys (list): A list of keys being compared with the queryword based in its similarity. Internally\n",
        "        keys will be a matrix of stacked embeddings of each provided word.\n",
        "        values (list): Each value being mapped by the keys.\n",
        "\n",
        "    Returns:\n",
        "        A matrix with all the computed attention scores.\n",
        "    \"\"\"\n",
        "    q = getWordEmbedding(queryWord)\n",
        "    k = np.array([getWordEmbedding(key) for key in keys])\n",
        "    v = values\n",
        "\n",
        "    attentionScore = computeAttention(q, k, v)\n",
        "\n",
        "    return attentionScore\n",
        "\n",
        "print(searchByMeaning(\"país\", [\"méxico\", \"alemania\", \"inglaterra\", \"perro\"], [10, 5, 2, 4]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEK0Z_d0Kj1h",
        "outputId": "9b170ae3-48e6-4be0-90dd-b74095fbee6d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1743777856094124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementación de Ecuación de Atención con Numpy\n",
        "El siguiente bloque de código es una **implementación completa de la Ecuación de Atención** descrita en el documento que define la Arquitectura Transformer aprovechando la libreria numpy, como puede verse el código es realmente sencillo y basicamente queda totalmente expresada utilizando la siguiente linea de código.\n",
        "\n",
        "`softmax(Q @ K.T / np.sqrt(dimension)) @ V`"
      ],
      "metadata": {
        "id": "B4AxeP2m9GF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    Implementation of softmax equation with numpy library.\n",
        "\n",
        "    Parameters:\n",
        "        x (matrix): A matrix.\n",
        "\n",
        "    Returns:\n",
        "        Normalized matrix, any element in the matrix is real value in following\n",
        "        range 0<= item <=1.\n",
        "    \"\"\"\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
        "\n",
        "def attention(Q, K, V):\n",
        "    \"\"\"\n",
        "    Computes the attention score, evaluate the scaled dot product attention equation\n",
        "    computing the dot products of the Q(uery) with all the K(eys) transpose, divide each by\n",
        "    sqrt(dimension) then apply a softmax function to obtain normalized weights.\n",
        "\n",
        "    Parameters:\n",
        "        Q (matrix): Q(uery) matrix, basically a matrix where each row is the\n",
        "        embedding being analyzed.\n",
        "        K (matrix): K(eys) matrix, same matrix than Q however the transpose is used\n",
        "        in order to compute a dod product where word is compared each other for similarity\n",
        "        taking advantage of dot product between matrix.\n",
        "        V (matrix): V(values) from the attention equation\n",
        "    \"\"\"\n",
        "\n",
        "    # It is expected that Q, K, and V has the same dimentionality, however,  we\n",
        "    # are not enforcing it with any validation.\n",
        "    dimension = K.shape[-1]\n",
        "\n",
        "    return softmax(Q @ K.T / np.sqrt(dimension)) @ V"
      ],
      "metadata": {
        "id": "yYMRs1JLRT0g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}